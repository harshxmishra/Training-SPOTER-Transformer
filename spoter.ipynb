{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-17T12:49:53.835965Z","iopub.status.busy":"2024-07-17T12:49:53.835142Z","iopub.status.idle":"2024-07-17T12:49:53.841130Z","shell.execute_reply":"2024-07-17T12:49:53.840037Z","shell.execute_reply.started":"2024-07-17T12:49:53.835924Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["# Video id and Label Df "]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.434875Z","iopub.status.busy":"2024-07-17T12:50:03.434384Z","iopub.status.idle":"2024-07-17T12:50:03.442405Z","shell.execute_reply":"2024-07-17T12:50:03.441301Z","shell.execute_reply.started":"2024-07-17T12:50:03.434840Z"},"trusted":true},"outputs":[],"source":["# !pip install mediapipe"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.443957Z","iopub.status.busy":"2024-07-17T12:50:03.443641Z","iopub.status.idle":"2024-07-17T12:50:03.451743Z","shell.execute_reply":"2024-07-17T12:50:03.450817Z","shell.execute_reply.started":"2024-07-17T12:50:03.443925Z"},"trusted":true},"outputs":[],"source":["# pd.set_option('display.max_colwidth', None)\n","\n","# # Define the folder containing the dataset\n","# folder_path = \"/kaggle/input/include\"\n","\n","# # Lists to store video paths and labels\n","# video_files = []\n","# labels = []\n","\n","# # Read the folder structure\n","# for root, dirs, files in os.walk(folder_path):\n","#     for file in files:\n","#         if file.endswith(\".MOV\"):  # Check for video files\n","#             video_path = os.path.join(root, file)\n","#             video_files.append(video_path)\n","\n","#             # Extract the label from the folder name\n","#             # Assuming the folder names are like \"Adjectives_1of8\"\n","#             label = os.path.basename(root).replace('_', ' ')\n","#             labels.append(label.split(\" \")[-1])\n","\n","# # Create a DataFrame\n","# xdf = pd.DataFrame({\"video_name\": video_files, \"tag\": labels})\n","\n","# # Display the DataFrame\n","# print(xdf.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["# Video Preprocessing and Conversion to CSV file"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.479318Z","iopub.status.busy":"2024-07-17T12:50:03.478898Z","iopub.status.idle":"2024-07-17T12:50:03.487296Z","shell.execute_reply":"2024-07-17T12:50:03.486476Z","shell.execute_reply.started":"2024-07-17T12:50:03.479284Z"},"trusted":true},"outputs":[],"source":["# BODY_IDENTIFIERS = {\n","#     \"nose\": 0,\n","#     \"neck\": -1,\n","#     \"rightEye\": 5,\n","#     \"leftEye\": 2,\n","#     \"rightEar\": 8,\n","#     \"leftEar\": 7,\n","#     \"rightShoulder\": 12,\n","#     \"leftShoulder\": 11,\n","#     \"rightElbow\": 14,\n","#     \"leftElbow\": 13,\n","#     \"rightWrist\": 16,\n","#     \"leftWrist\": 15\n","# }\n","# HAND_IDENTIFIERS = {\n","#     \"wrist\": 0,\n","#     \"indexTip\": 8,\n","#     \"indexDIP\": 7,\n","#     \"indexPIP\": 6,\n","#     \"indexMCP\": 5,\n","#     \"middleTip\": 12,\n","#     \"middleDIP\": 11,\n","#     \"middlePIP\": 10,\n","#     \"middleMCP\": 9,\n","#     \"ringTip\": 16,\n","#     \"ringDIP\": 15,\n","#     \"ringPIP\": 14,\n","#     \"ringMCP\": 13,\n","#     \"littleTip\": 20,\n","#     \"littleDIP\": 19,\n","#     \"littlePIP\": 18,\n","#     \"littleMCP\": 17,\n","#     \"thumbTip\": 4,\n","#     \"thumbIP\": 3,\n","#     \"thumbMP\": 2,\n","#     \"thumbCMC\": 1\n","# }\n","\n","# class mp_holistic_data:\n","#     def __init__(self, column_names):\n","#         self.data_hub = {}\n","#         for n in column_names[1:-1]:\n","#             self.data_hub[n] = []\n","\n","#     def hand_append_zero(self, handedness):\n","#         for k in self.data_hub.keys():\n","#             if \"_\" + handedness + \"_\" in k:\n","#                 self.data_hub[k].append(0)\n","\n","#     def hand_append_value(self, handedness, hand_landmarks):\n","#         for name, lm_idx in HAND_IDENTIFIERS.items():\n","#             lm = hand_landmarks.landmark[lm_idx]\n","#             for xy, xy_value in zip(['_X', '_Y'], [lm.x, lm.y]):\n","#                 k = name + '_' + handedness + xy\n","#                 self.data_hub[k].append(xy_value)\n","\n","#     def get_series(self):\n","#         return pd.Series(self.data_hub)\n","\n","#     def extract_data(self, holistic_results):\n","#         def neck(pose_results):\n","#             ls = pose_results.pose_landmarks.landmark[11]\n","#             rs = pose_results.pose_landmarks.landmark[12]\n","#             no = pose_results.pose_landmarks.landmark[0]\n","#             if (ls.visibility > 0.5) & (rs.visibility > 0.5) & (no.visibility > 0.5):\n","#                 # This indicates the neck better. But it does not affect the result.\n","#                 cx = (ls.x + rs.x) / 2\n","#                 cy = (ls.y + rs.y) / 2\n","#                 dx = no.x - cx\n","#                 dy = no.y - cy\n","#                 x = cx + 0.3 * dx\n","#                 y = cy + 0.3 * dy\n","               \n","#             else:\n","#                 x = 0\n","#                 y = 0\n","#             return [x, y]\n","\n","#         # for the frame that can not extract skeleton from\n","#         if not holistic_results.pose_landmarks:\n","#             return\n","#         for name, lm_idx in BODY_IDENTIFIERS.items():\n","#             if name == \"neck\":\n","#                 xy_value = neck(holistic_results)\n","#             else:\n","#                 lm = holistic_results.pose_landmarks.landmark[lm_idx]\n","#                 visible = float(lm.visibility >= 0.5)\n","#                 xy_value = [lm.x * visible, lm.y * visible]\n","#             for xy_id, xy in zip(['_X', '_Y'], xy_value):\n","#                 s_name = name + xy_id\n","#                 self.data_hub[s_name].append(xy)\n","\n","#         for handedness, lm in zip(['Right', 'Left'],\n","#                                   [holistic_results.right_hand_landmarks, holistic_results.left_hand_landmarks]):\n","#             if lm:\n","#                 self.hand_append_value(handedness, lm)\n","#             else:\n","#                 self.hand_append_zero(handedness)\n","#         return"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.488776Z","iopub.status.busy":"2024-07-17T12:50:03.488474Z","iopub.status.idle":"2024-07-17T12:50:03.501909Z","shell.execute_reply":"2024-07-17T12:50:03.500779Z","shell.execute_reply.started":"2024-07-17T12:50:03.488744Z"},"trusted":true},"outputs":[],"source":["# import warnings\n","\n","# import pandas as pd\n","# from os import path\n","# import cv2\n","# import mediapipe as mp\n","# import json\n","\n","\n","# mp_holistic = mp.solutions.holistic\n","\n","# holistic = mp_holistic.Holistic()\n","\n","# column_names = []\n","# column_names.append('video_id')\n","# for id_name in BODY_IDENTIFIERS.keys():\n","#     for xy in [\"_X\", \"_Y\"]:\n","#         column_names.append(id_name + xy)\n","\n","# for lr in [\"_Right\", \"_Left\"]:\n","#     for id_name in HAND_IDENTIFIERS.keys():\n","#         for xy in [\"_X\", \"_Y\"]:\n","#             column_names.append(id_name + lr + xy)\n","\n","# column_names.append('labels')\n","\n","\n","# def create_df(flnm, column_names):\n","#     df = pd.DataFrame(columns=column_names)\n","#     return df\n","\n","\n","# def save_data(df, data, flnm):\n","#     series_data = data.get_series()\n","#     series_data = series_data.reindex(df.columns, fill_value=pd.NA)  # Ensure the Series aligns with the DataFrame columns\n","#     new_row_df = pd.DataFrame([series_data])\n","#     df = pd.concat([df, new_row_df], ignore_index=True)\n","#     return df\n","# #     df.to_pickle(flnm)\n","\n","\n","# def obtain_pose_data(path):\n","#     cap = cv2.VideoCapture(path)\n","#     data = mp_holistic_data(column_names)\n","#     while cap.isOpened():\n","#         ret, frame = cap.read()\n","#         if not ret:\n","#             break\n","#         # Recolor image to RGB\n","#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","#         # Make detection\n","#         holistic_results = holistic.process(image)\n","#         # Extract feature and save to mp_pose_data class\n","#         data.extract_data(holistic_results)\n","#     cap.release()\n","\n","#     return data"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.503628Z","iopub.status.busy":"2024-07-17T12:50:03.503280Z","iopub.status.idle":"2024-07-17T12:50:03.515839Z","shell.execute_reply":"2024-07-17T12:50:03.514849Z","shell.execute_reply.started":"2024-07-17T12:50:03.503602Z"},"trusted":true},"outputs":[],"source":["# import tqdm\n","# from tqdm.notebook import tqdm_notebook\n","\n","# def process_videos(xdf, df):\n","#     for index, row in tqdm_notebook(xdf.iterrows(), total=len(xdf), unit='videos', desc='Processing Videos'):\n","#         video_path = row['video_name']\n","#         label = row['tag']\n","# #         print(video_path)\n","#         # Obtain pose data from video\n","#         data = obtain_pose_data(video_path)\n","        \n","#         # Save data to main DataFrame\n","#         df = save_data(df, data, video_path)\n","        \n","        \n","#         # Update video_id and labels for the processed video\n","#         df.loc[df['video_id'].isna(), 'video_id'] = video_path\n","#         df.loc[df['labels'].isna(), 'labels'] = label\n","        \n","#     return df"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.517353Z","iopub.status.busy":"2024-07-17T12:50:03.516901Z","iopub.status.idle":"2024-07-17T12:50:03.529585Z","shell.execute_reply":"2024-07-17T12:50:03.528454Z","shell.execute_reply.started":"2024-07-17T12:50:03.517323Z"},"trusted":true},"outputs":[],"source":["# df = create_df(\"flnm\", column_names)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.531179Z","iopub.status.busy":"2024-07-17T12:50:03.530813Z","iopub.status.idle":"2024-07-17T12:50:03.539245Z","shell.execute_reply":"2024-07-17T12:50:03.538298Z","shell.execute_reply.started":"2024-07-17T12:50:03.531152Z"},"trusted":true},"outputs":[],"source":["# df = process_videos(xdf, df)"]},{"cell_type":"markdown","metadata":{},"source":["# Data Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for i in df['video_id']:\n","\n","#     if len(i.split('/'))>8:\n","# #         print(i.split('/'))\n","# #         print(i.split('/')[-3])\n","#         df.loc[df['video_id'] == i, 'labels'] = i.split('/')[-3]\n","#     else:\n","# #         print(i.split('/'))\n","# #         print(i.split('/')[-2])\n","#         df.loc[df['video_id'] == i, 'labels'] = i.split('/')[-2]\n","# # df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import re\n","# for i in df['labels']:\n","#     o = i.split(\".\")[-1].strip()\n","#     k = re.sub(r'[^a-z0-9]', '', o.lower())\n","#     df.loc[df['labels'] == i, 'labels'] = k"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dict_list = {j: i for i, j in enumerate(df['labels'].unique())}    \n","# df['labels'] = df['labels'].map(dict_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df = df.drop(['video_id'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df.to_csv(\"/kaggle/working/spoter.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv('spoter.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df = df[df['labels'].between(0,19)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["262\n"]}],"source":["def compute_num_labels(df):\n","    num_labels = df['labels'].nunique()\n","    return num_labels\n","\n","print(compute_num_labels(df))"]},{"cell_type":"markdown","metadata":{},"source":["# Normalisation"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.569359Z","iopub.status.busy":"2024-07-17T12:50:03.569051Z","iopub.status.idle":"2024-07-17T12:50:03.597969Z","shell.execute_reply":"2024-07-17T12:50:03.596752Z","shell.execute_reply.started":"2024-07-17T12:50:03.569333Z"},"trusted":true},"outputs":[],"source":["import logging\n","import pandas as pd\n","\n","HAND_IDENTIFIERS = [\n","    \"wrist\",\n","    \"indexTip\",\n","    \"indexDIP\",\n","    \"indexPIP\",\n","    \"indexMCP\",\n","    \"middleTip\",\n","    \"middleDIP\",\n","    \"middlePIP\",\n","    \"middleMCP\",\n","    \"ringTip\",\n","    \"ringDIP\",\n","    \"ringPIP\",\n","    \"ringMCP\",\n","    \"littleTip\",\n","    \"littleDIP\",\n","    \"littlePIP\",\n","    \"littleMCP\",\n","    \"thumbTip\",\n","    \"thumbIP\",\n","    \"thumbMP\",\n","    \"thumbCMC\"\n","]\n","\n","\n","def normalize_hands_full(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Normalizes the hands position data using the Bohacek-normalization algorithm.\n","    :param df: pd.DataFrame to be normalized\n","    :return: pd.DataFrame with normalized values for hand pose\n","    \"\"\"\n","\n","    # TODO: Fix division by zero\n","    df.columns = [item.replace(\"_Left_\", \"_0_\").replace(\"_Right_\", \"_1_\") for item in list(df.columns)]\n","\n","    normalized_df = pd.DataFrame(columns=df.columns)\n","\n","    hand_landmarks = {\"X\": {0: [], 1: []}, \"Y\": {0: [], 1: []}}\n","\n","    # Determine how many hands are present in the dataset\n","    range_hand_size = 1\n","    if \"wrist_1_X\" in df.columns:\n","        range_hand_size = 2\n","\n","    # Construct the relevant identifiers\n","    for identifier in HAND_IDENTIFIERS:\n","        for hand_index in range(range_hand_size):\n","            hand_landmarks[\"X\"][hand_index].append(identifier + \"_\" + str(hand_index) + \"_X\")\n","            hand_landmarks[\"Y\"][hand_index].append(identifier + \"_\" + str(hand_index) + \"_Y\")\n","\n","    # Iterate over all of the records in the dataset\n","    for index, row in df.iterrows():\n","        # Treat each hand individually\n","        for hand_index in range(range_hand_size):\n","\n","            sequence_size = len(row[\"wrist_\" + str(hand_index) + \"_X\"])\n","\n","            # Treat each element of the sequence (analyzed frame) individually\n","            for sequence_index in range(sequence_size):\n","\n","                # Retrieve all of the X and Y values of the current frame\n","                landmarks_x_values = [row[key][sequence_index] for key in hand_landmarks[\"X\"][hand_index] if row[key][sequence_index] != 0]\n","                landmarks_y_values = [row[key][sequence_index] for key in hand_landmarks[\"Y\"][hand_index] if row[key][sequence_index] != 0]\n","\n","                # Prevent from even starting the analysis if some necessary elements are not present\n","                if not landmarks_x_values or not landmarks_y_values:\n","                    logging.warning(\n","                        \" HAND LANDMARKS: One frame could not be normalized as there is no data present. Record: \" + str(index) +\n","                        \", Frame: \" + str(sequence_index))\n","                    continue\n","\n","                # Calculate the deltas\n","                width, height = max(landmarks_x_values) - min(landmarks_x_values), max(landmarks_y_values) - min(landmarks_y_values)\n","                if width > height:\n","                    delta_x = 0.1 * width\n","                    delta_y = delta_x + ((width - height) / 2)\n","                else:\n","                    delta_y = 0.1 * height\n","                    delta_x = delta_y + ((height - width) / 2)\n","\n","                # Set the starting and ending point of the normalization bounding box\n","                starting_point = (min(landmarks_x_values) - delta_x, min(landmarks_y_values) - delta_y)\n","                ending_point = (max(landmarks_x_values) + delta_x, max(landmarks_y_values) + delta_y)\n","\n","                # Normalize individual landmarks and save the results\n","                for identifier in HAND_IDENTIFIERS:\n","                    key = identifier + \"_\" + str(hand_index) + \"_\"\n","\n","                    # Prevent from trying to normalize incorrectly captured points\n","                    if row[key + \"X\"][sequence_index] == 0 or (ending_point[0] - starting_point[0]) == 0 or (starting_point[1] - ending_point[1]) == 0:\n","                        continue\n","\n","                    normalized_x = (row[key + \"X\"][sequence_index] - starting_point[0]) / (ending_point[0] -\n","                                                                                           starting_point[0])\n","                    normalized_y = (row[key + \"Y\"][sequence_index] - ending_point[1]) / (starting_point[1] -\n","                                                                                         ending_point[1])\n","\n","                    row[key + \"X\"][sequence_index] = normalized_x\n","                    row[key + \"Y\"][sequence_index] = normalized_y\n","        row = pd.DataFrame([row])\n","        normalized_df = pd.concat([normalized_df, row], ignore_index=True)\n","\n","    return normalized_df\n","\n","\n","def normalize_single_hand_dict(row: dict):\n","    \"\"\"\n","    Normalizes the skeletal data for a given sequence of frames with signer's hand pose data. The normalization follows\n","    the definition from our paper.\n","    :param row: Dictionary containing key-value pairs with joint identifiers and corresponding lists (sequences) of\n","                that particular joints coordinates\n","    :return: Dictionary with normalized skeletal data (following the same schema as input data)\n","    \"\"\"\n","\n","    hand_landmarks = {0: [], 1: []}\n","\n","    # Determine how many hands are present in the dataset\n","    range_hand_size = 1\n","    if \"wrist_1\" in row.keys():\n","        range_hand_size = 2\n","\n","    # Construct the relevant identifiers\n","    for identifier in HAND_IDENTIFIERS:\n","        for hand_index in range(range_hand_size):\n","            hand_landmarks[hand_index].append(identifier + \"_\" + str(hand_index))\n","\n","    # Treat each hand individually\n","    for hand_index in range(range_hand_size):\n","\n","        sequence_size = len(row[\"wrist_\" + str(hand_index)])\n","\n","        # Treat each element of the sequence (analyzed frame) individually\n","        for sequence_index in range(sequence_size):\n","\n","            # Retrieve all of the X and Y values of the current frame\n","            landmarks_x_values = [row[key][sequence_index][0] for key in hand_landmarks[hand_index] if\n","                                  row[key][sequence_index][0] != 0]\n","            landmarks_y_values = [row[key][sequence_index][1] for key in hand_landmarks[hand_index] if\n","                                  row[key][sequence_index][1] != 0]\n","\n","            # Prevent from even starting the analysis if some necessary elements are not present\n","            if not landmarks_x_values or not landmarks_y_values:\n","                continue\n","\n","            # Calculate the deltas\n","            width, height = max(landmarks_x_values) - min(landmarks_x_values), max(landmarks_y_values) - min(landmarks_y_values)\n","            if width > height:\n","                delta_x = 0.1 * width\n","                delta_y = delta_x + ((width - height) / 2)\n","            else:\n","                delta_y = 0.1 * height\n","                delta_x = delta_y + ((height - width) / 2)\n","\n","            # Set the starting and ending point of the normalization bounding box\n","            starting_point = [min(landmarks_x_values) - delta_x, min(landmarks_y_values) - delta_y]\n","            ending_point = [max(landmarks_x_values) + delta_x, max(landmarks_y_values) + delta_y]\n","            # Ensure that all of the bounding-box-defining coordinates are not out of the picture\n","            if starting_point[0] < 0: starting_point[0] = 0\n","            if starting_point[1] > 1: starting_point[1] = 1\n","            if ending_point[0] < 0: ending_point[0] = 0\n","            if ending_point[1] > 1: ending_point[1] = 1\n","\n","            # Normalize individual landmarks and save the results\n","            for identifier in HAND_IDENTIFIERS:\n","                key = identifier + \"_\" + str(hand_index)\n","\n","                # Prevent from trying to normalize incorrectly captured points\n","                if row[key][sequence_index][0] == 0 or (ending_point[0] - starting_point[0]) == 0 or (starting_point[1] - ending_point[1]) == 0:\n","                    continue\n","\n","                normalized_x = (row[key][sequence_index][0] - starting_point[0]) / (ending_point[0] - starting_point[0])\n","                normalized_y = (row[key][sequence_index][1] - starting_point[1]) / (ending_point[1] - starting_point[1])\n","\n","                row[key][sequence_index] = list(row[key][sequence_index])\n","\n","                row[key][sequence_index][0] = normalized_x\n","                row[key][sequence_index][1] = normalized_y\n","\n","    return row\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.599894Z","iopub.status.busy":"2024-07-17T12:50:03.599558Z","iopub.status.idle":"2024-07-17T12:50:03.633029Z","shell.execute_reply":"2024-07-17T12:50:03.631910Z","shell.execute_reply.started":"2024-07-17T12:50:03.599866Z"},"trusted":true},"outputs":[],"source":["import logging\n","import pandas as pd\n","\n","BODY_IDENTIFIERS = [\n","    \"nose\",\n","    \"neck\",\n","    \"rightEye\",\n","    \"leftEye\",\n","    \"rightEar\",\n","    \"leftEar\",\n","    \"rightShoulder\",\n","    \"leftShoulder\",\n","    \"rightElbow\",\n","    \"leftElbow\",\n","    \"rightWrist\",\n","    \"leftWrist\"\n","]\n","\n","\n","def normalize_body_full(df: pd.DataFrame) -> (pd.DataFrame, list):\n","    \"\"\"\n","    Normalizes the body position data using the Bohacek-normalization algorithm.\n","    :param df: pd.DataFrame to be normalized\n","    :return: pd.DataFrame with normalized values for body pose\n","    \"\"\"\n","\n","    # TODO: Fix division by zero\n","\n","    normalized_df = pd.DataFrame(columns=df.columns)\n","    invalid_row_indexes = []\n","    body_landmarks = {\"X\": [], \"Y\": []}\n","\n","    # Construct the relevant identifiers\n","    for identifier in BODY_IDENTIFIERS:\n","        body_landmarks[\"X\"].append(identifier + \"_X\")\n","        body_landmarks[\"Y\"].append(identifier + \"_Y\")\n","\n","    # Iterate over all of the records in the dataset\n","    for index, row in df.iterrows():\n","\n","        sequence_size = len(row[\"leftEar_Y\"])\n","        valid_sequence = True\n","        original_row = row\n","\n","        last_starting_point, last_ending_point = None, None\n","\n","        # Treat each element of the sequence (analyzed frame) individually\n","        for sequence_index in range(sequence_size):\n","\n","            # Prevent from even starting the analysis if some necessary elements are not present\n","            if (row[\"leftShoulder_X\"][sequence_index] == 0 or row[\"rightShoulder_X\"][sequence_index] == 0) and (row[\"neck_X\"][sequence_index] == 0 or row[\"nose_X\"][sequence_index] == 0):\n","                if not last_starting_point:\n","                    valid_sequence = False\n","                    continue\n","\n","                else:\n","                    starting_point, ending_point = last_starting_point, last_ending_point\n","\n","            else:\n","\n","                # NOTE:\n","                #\n","                # While in the paper, it is written that the head metric is calculated by halving the shoulder distance,\n","                # this is meant for the distance between the very ends of one's shoulder, as literature studying body\n","                # metrics and ratios generally states. The Vision Pose Estimation API, however, seems to be predicting\n","                # rather the center of one's shoulder. Based on our experiments and manual reviews of the data, employing\n","                # this as just the plain shoulder distance seems to be more corresponding to the desired metric.\n","                #\n","                # Please, review this if using other third-party pose estimation libraries.\n","\n","                if row[\"leftShoulder_X\"][sequence_index] != 0 and row[\"rightShoulder_X\"][sequence_index] != 0:\n","                    left_shoulder = (row[\"leftShoulder_X\"][sequence_index], row[\"leftShoulder_Y\"][sequence_index])\n","                    right_shoulder = (row[\"rightShoulder_X\"][sequence_index], row[\"rightShoulder_Y\"][sequence_index])\n","                    shoulder_distance = ((((left_shoulder[0] - right_shoulder[0]) ** 2) + (\n","                                (left_shoulder[1] - right_shoulder[1]) ** 2)) ** 0.5)\n","                    head_metric = shoulder_distance\n","                else:\n","                    neck = (row[\"neck_X\"][sequence_index], row[\"neck_Y\"][sequence_index])\n","                    nose = (row[\"nose_X\"][sequence_index], row[\"nose_Y\"][sequence_index])\n","                    neck_nose_distance = ((((neck[0] - nose[0]) ** 2) + ((neck[1] - nose[1]) ** 2)) ** 0.5)\n","                    head_metric = neck_nose_distance\n","\n","                # Set the starting and ending point of the normalization bounding box\n","                starting_point = [row[\"neck_X\"][sequence_index] - 3 * head_metric, row[\"leftEye_Y\"][sequence_index] + (head_metric / 2)]\n","                ending_point = [row[\"neck_X\"][sequence_index] + 3 * head_metric, starting_point[1] - 6 * head_metric]\n","\n","                last_starting_point, last_ending_point = starting_point, ending_point\n","\n","            # Ensure that all of the bounding-box-defining coordinates are not out of the picture\n","            if starting_point[0] < 0: starting_point[0] = 0\n","            if starting_point[1] < 0: starting_point[1] = 0\n","            if ending_point[0] < 0: ending_point[0] = 0\n","            if ending_point[1] < 0: ending_point[1] = 0\n","\n","            # Normalize individual landmarks and save the results\n","            for identifier in BODY_IDENTIFIERS:\n","                key = identifier + \"_\"\n","\n","                # Prevent from trying to normalize incorrectly captured points\n","                if row[key + \"X\"][sequence_index] == 0:\n","                    continue\n","\n","                normalized_x = (row[key + \"X\"][sequence_index] - starting_point[0]) / (ending_point[0] -\n","                                                                                       starting_point[0])\n","                normalized_y = (row[key + \"Y\"][sequence_index] - ending_point[1]) / (starting_point[1] -\n","                                                                                       ending_point[1])\n","\n","                row[key + \"X\"][sequence_index] = normalized_x\n","                row[key + \"Y\"][sequence_index] = normalized_y\n","\n","        if valid_sequence:\n","            row = pd.DataFrame([row])\n","            normalized_df = pd.concat([normalized_df, row], ignore_index=True)\n","        else:\n","            logging.warning(\" BODY LANDMARKS: One video instance could not be normalized.\")\n","            row = pd.DataFrame([row])\n","            normalized_df = pd.concat([normalized_df, original_row], ignore_index=True)\n","            invalid_row_indexes.append(index)\n","\n","    print(\"The normalization of body is finished.\")\n","    print(\"\\t-> Original size:\", df.shape[0])\n","    print(\"\\t-> Normalized size:\", normalized_df.shape[0])\n","    print(\"\\t-> Problematic videos:\", len(invalid_row_indexes))\n","\n","    return normalized_df, invalid_row_indexes\n","\n","\n","def normalize_single_body_dict(row: dict):\n","    \"\"\"\n","    Normalizes the skeletal data for a given sequence of frames with signer's body pose data. The normalization follows\n","    the definition from our paper.\n","    :param row: Dictionary containing key-value pairs with joint identifiers and corresponding lists (sequences) of\n","                that particular joints coordinates\n","    :return: Dictionary with normalized skeletal data (following the same schema as input data)\n","    \"\"\"\n","\n","    sequence_size = len(row[\"leftEar\"])\n","    valid_sequence = True\n","    original_row = row\n","\n","    last_starting_point, last_ending_point = None, None\n","\n","    # Treat each element of the sequence (analyzed frame) individually\n","    for sequence_index in range(sequence_size):\n","\n","        # Prevent from even starting the analysis if some necessary elements are not present\n","        if (row[\"leftShoulder\"][sequence_index][0] == 0 or row[\"rightShoulder\"][sequence_index][0] == 0) and (\n","                row[\"neck\"][sequence_index][0] == 0 or row[\"nose\"][sequence_index][0] == 0):\n","            if not last_starting_point:\n","                valid_sequence = False\n","                continue\n","\n","            else:\n","                starting_point, ending_point = last_starting_point, last_ending_point\n","\n","        else:\n","\n","            # NOTE:\n","            #\n","            # While in the paper, it is written that the head metric is calculated by halving the shoulder distance,\n","            # this is meant for the distance between the very ends of one's shoulder, as literature studying body\n","            # metrics and ratios generally states. The Vision Pose Estimation API, however, seems to be predicting\n","            # rather the center of one's shoulder. Based on our experiments and manual reviews of the data, employing\n","            # this as just the plain shoulder distance seems to be more corresponding to the desired metric.\n","            #\n","            # Please, review this if using other third-party pose estimation libraries.\n","\n","            if row[\"leftShoulder\"][sequence_index][0] != 0 and row[\"rightShoulder\"][sequence_index][0] != 0:\n","                left_shoulder = (row[\"leftShoulder\"][sequence_index][0], row[\"leftShoulder\"][sequence_index][1])\n","                right_shoulder = (row[\"rightShoulder\"][sequence_index][0], row[\"rightShoulder\"][sequence_index][1])\n","                shoulder_distance = ((((left_shoulder[0] - right_shoulder[0]) ** 2) + (\n","                        (left_shoulder[1] - right_shoulder[1]) ** 2)) ** 0.5)\n","                head_metric = shoulder_distance\n","            else:\n","                neck = (row[\"neck\"][sequence_index][0], row[\"neck\"][sequence_index][1])\n","                nose = (row[\"nose\"][sequence_index][0], row[\"nose\"][sequence_index][1])\n","                neck_nose_distance = ((((neck[0] - nose[0]) ** 2) + ((neck[1] - nose[1]) ** 2)) ** 0.5)\n","                head_metric = neck_nose_distance\n","\n","            # Set the starting and ending point of the normalization bounding box\n","            #starting_point = [row[\"neck\"][sequence_index][0] - 3 * head_metric,\n","            #                 row[\"leftEye\"][sequence_index][1] + (head_metric / 2)]\n","            starting_point = [row[\"neck\"][sequence_index][0] - 1 * head_metric,\n","                            row[\"leftEye\"][sequence_index][1] - head_metric/2]\n","            ending_point = [row[\"neck\"][sequence_index][0] + 1 * head_metric,\n","                            starting_point[1] + 3 * head_metric]\n","\n","            last_starting_point, last_ending_point = starting_point, ending_point\n","\n","        # Ensure that all of the bounding-box-defining coordinates are not out of the picture\n","        if starting_point[0] < 0: starting_point[0] = 0\n","        if starting_point[1] > 1: starting_point[1] = 1\n","        if ending_point[0] < 0: ending_point[0] = 0\n","        if ending_point[1] > 1: ending_point[1] = 1\n","\n","        # Normalize individual landmarks and save the results\n","        for identifier in BODY_IDENTIFIERS:\n","            key = identifier\n","\n","            # Prevent from trying to normalize incorrectly captured points\n","            if row[key][sequence_index][0] == 0:\n","                continue\n","\n","            if (ending_point[0] - starting_point[0]) == 0 or (starting_point[1] - ending_point[1]) == 0:\n","                logging.info(\"Problematic normalization\")\n","                valid_sequence = False\n","                break\n","\n","            normalized_x = (row[key][sequence_index][0] - starting_point[0]) / (ending_point[0] - starting_point[0])\n","            normalized_y = (row[key][sequence_index][1] - starting_point[1]) / (ending_point[1] - starting_point[1])\n","\n","            row[key][sequence_index] = list(row[key][sequence_index])\n","\n","            row[key][sequence_index][0] = normalized_x\n","            row[key][sequence_index][1] = normalized_y\n","\n","    if valid_sequence:\n","        return row\n","\n","    else:\n","        return original_row\n","\n"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.634643Z","iopub.status.busy":"2024-07-17T12:50:03.634320Z","iopub.status.idle":"2024-07-17T12:50:03.646795Z","shell.execute_reply":"2024-07-17T12:50:03.645801Z","shell.execute_reply.started":"2024-07-17T12:50:03.634608Z"},"trusted":true},"outputs":[],"source":["# import ast\n","# import pandas as pd\n","\n","\n","# # Load the dataset\n","# # df = pd.read_csv(\"/Users/matyasbohacek/Documents/WLASL_test_15fps.csv\", encoding=\"utf-8\")\n","\n","# # Retrieve metadata\n","# # video_size_heights = df[\"video_size_height\"].to_list()\n","# # video_size_widths = df[\"video_size_width\"].to_list()\n","\n","# # Delete redundant (non-related) properties\n","# # del df[\"video_size_height\"]\n","# # del df[\"video_size_width\"]\n","\n","# # Temporarily remove other relevant metadata\n","# labels = df[\"labels\"].to_list()\n","# # video_fps = df[\"video_fps\"].to_list()\n","# del df[\"labels\"]\n","# video_id = df[\"video_id\"].to_list()\n","# del df[\"video_id\"]\n","\n","# # Convert the strings into lists\n","# convert = lambda x: ast.literal_eval(str(x))\n","# for column in xdf.columns:\n","#     xdf[column] = xdf[column].apply(convert)\n","\n","# # Perform the normalizations\n","# df = normalize_hands_full(df)\n","# df, invalid_row_indexes = normalize_body_full(df)\n","\n","# # Clear lists of items from deleted rows\n","# # labels = [t for i, t in enumerate(labels) if i not in invalid_row_indexes]\n","# # video_fps = [t for i, t in enumerate(video_fps) if i not in invalid_row_indexes]\n","\n","# # Return the metadata back to the dataset\n","# xdf[\"labels\"] = labels\n","# # df[\"video_fps\"] = video_fps\n","# df\n","# # df.to_csv(\"/Users/matyasbohacek/Desktop/WLASL_test_15fps_normalized.csv\", encoding=\"utf-8\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Augmentation"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.648631Z","iopub.status.busy":"2024-07-17T12:50:03.648318Z","iopub.status.idle":"2024-07-17T12:50:03.680624Z","shell.execute_reply":"2024-07-17T12:50:03.679511Z","shell.execute_reply.started":"2024-07-17T12:50:03.648605Z"},"trusted":true},"outputs":[],"source":["import math\n","import logging\n","import cv2\n","import random\n","\n","import numpy as np\n","\n","\n","ARM_IDENTIFIERS_ORDER = [\"neck\", \"$side$Shoulder\", \"$side$Elbow\", \"$side$Wrist\"]\n","\n","\n","def __random_pass(prob):\n","    return random.random() < prob\n","\n","\n","def __numpy_to_dictionary(data_array: np.ndarray) -> dict:\n","    \"\"\"\n","    Supplementary method converting a NumPy array of body landmark data into dictionaries. The array data must match the\n","    order of the BODY_IDENTIFIERS list.\n","    \"\"\"\n","\n","    output = {}\n","\n","    for landmark_index, identifier in enumerate(BODY_IDENTIFIERS):\n","        output[identifier] = data_array[:, landmark_index].tolist()\n","\n","    return output\n","\n","\n","def __dictionary_to_numpy(landmarks_dict: dict) -> np.ndarray:\n","    \"\"\"\n","    Supplementary method converting dictionaries of body landmark data into respective NumPy arrays. The resulting array\n","    will match the order of the BODY_IDENTIFIERS list.\n","    \"\"\"\n","\n","    output = np.empty(shape=(len(landmarks_dict[\"leftEar\"]), len(BODY_IDENTIFIERS), 2))\n","\n","    for landmark_index, identifier in enumerate(BODY_IDENTIFIERS):\n","        output[:, landmark_index, 0] = np.array(landmarks_dict[identifier])[:, 0]\n","        output[:, landmark_index, 1] = np.array(landmarks_dict[identifier])[:, 1]\n","\n","    return output\n","\n","\n","def __rotate(origin: tuple, point: tuple, angle: float):\n","    \"\"\"\n","    Rotates a point counterclockwise by a given angle around a given origin.\n","    :param origin: Landmark in the (X, Y) format of the origin from which to count angle of rotation\n","    :param point: Landmark in the (X, Y) format to be rotated\n","    :param angle: Angle under which the point shall be rotated\n","    :return: New landmarks (coordinates)\n","    \"\"\"\n","\n","    ox, oy = origin\n","    px, py = point\n","\n","    qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n","    qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n","\n","    return qx, qy\n","\n","\n","def __preprocess_row_sign(sign: dict) -> (dict, dict):\n","    \"\"\"\n","    Supplementary method splitting the single-dictionary skeletal data into two dictionaries of body and hand landmarks\n","    respectively.\n","    \"\"\"\n","    HAND_IDENTIFIERS = [\n","    \"wrist\",\n","    \"indexTip\",\n","    \"indexDIP\",\n","    \"indexPIP\",\n","    \"indexMCP\",\n","    \"middleTip\",\n","    \"middleDIP\",\n","    \"middlePIP\",\n","    \"middleMCP\",\n","    \"ringTip\",\n","    \"ringDIP\",\n","    \"ringPIP\",\n","    \"ringMCP\",\n","    \"littleTip\",\n","    \"littleDIP\",\n","    \"littlePIP\",\n","    \"littleMCP\",\n","    \"thumbTip\",\n","    \"thumbIP\",\n","    \"thumbMP\",\n","    \"thumbCMC\"\n","]\n","    HAND_IDENTIFIERS = [id + \"_0\" for id in HAND_IDENTIFIERS] + [id + \"_1\" for id in HAND_IDENTIFIERS]\n","\n","    sign_eval = sign\n","\n","    if \"nose_X\" in sign_eval:\n","        body_landmarks = {identifier: [(x, y) for x, y in zip(sign_eval[identifier + \"_X\"], sign_eval[identifier + \"_Y\"])]\n","                          for identifier in BODY_IDENTIFIERS}\n","        hand_landmarks = {identifier: [(x, y) for x, y in zip(sign_eval[identifier + \"_X\"], sign_eval[identifier + \"_Y\"])]\n","                          for identifier in HAND_IDENTIFIERS}\n","\n","    else:\n","        body_landmarks = {identifier: sign_eval[identifier] for identifier in BODY_IDENTIFIERS}\n","        hand_landmarks = {identifier: sign_eval[identifier] for identifier in HAND_IDENTIFIERS}\n","\n","    return body_landmarks, hand_landmarks\n","\n","\n","def __wrap_sign_into_row(body_identifiers: dict, hand_identifiers: dict) -> dict:\n","    \"\"\"\n","    Supplementary method for merging body and hand data into a single dictionary.\n","    \"\"\"\n","    HAND_IDENTIFIERS = [\n","    \"wrist\",\n","    \"indexTip\",\n","    \"indexDIP\",\n","    \"indexPIP\",\n","    \"indexMCP\",\n","    \"middleTip\",\n","    \"middleDIP\",\n","    \"middlePIP\",\n","    \"middleMCP\",\n","    \"ringTip\",\n","    \"ringDIP\",\n","    \"ringPIP\",\n","    \"ringMCP\",\n","    \"littleTip\",\n","    \"littleDIP\",\n","    \"littlePIP\",\n","    \"littleMCP\",\n","    \"thumbTip\",\n","    \"thumbIP\",\n","    \"thumbMP\",\n","    \"thumbCMC\"\n","]\n","    HAND_IDENTIFIERS = [id + \"_0\" for id in HAND_IDENTIFIERS] + [id + \"_1\" for id in HAND_IDENTIFIERS]\n","\n","    return {**body_identifiers, **hand_identifiers}\n","\n","\n","def augment_rotate(sign: dict, angle_range: tuple) -> dict:\n","    \"\"\"\n","    AUGMENTATION TECHNIQUE. All the joint coordinates in each frame are rotated by a random angle up to 13 degrees with\n","    the center of rotation lying in the center of the frame, which is equal to [0.5; 0.5].\n","    :param sign: Dictionary with sequential skeletal data of the signing person\n","    :param angle_range: Tuple containing the angle range (minimal and maximal angle in degrees) to randomly choose the\n","                        angle by which the landmarks will be rotated from\n","    :return: Dictionary with augmented (by rotation) sequential skeletal data of the signing person\n","    \"\"\"\n","\n","    body_landmarks, hand_landmarks = __preprocess_row_sign(sign)\n","    angle = math.radians(random.uniform(*angle_range))\n","\n","    body_landmarks = {key: [__rotate((0.5, 0.5), frame, angle) for frame in value] for key, value in\n","                      body_landmarks.items()}\n","    hand_landmarks = {key: [__rotate((0.5, 0.5), frame, angle) for frame in value] for key, value in\n","                      hand_landmarks.items()}\n","\n","    return __wrap_sign_into_row(body_landmarks, hand_landmarks)\n","\n","\n","def augment_shear(sign: dict, type: str, squeeze_ratio: tuple) -> dict:\n","    \"\"\"\n","    AUGMENTATION TECHNIQUE.\n","        - Squeeze. All the frames are squeezed from both horizontal sides. Two different random proportions up to 15% of\n","        the original frame's width for both left and right side are cut.\n","        - Perspective transformation. The joint coordinates are projected onto a new plane with a spatially defined\n","        center of projection, which simulates recording the sign video with a slight tilt. Each time, the right or left\n","        side, as well as the proportion by which both the width and height will be reduced, are chosen randomly. This\n","        proportion is selected from a uniform distribution on the [0; 1) interval. Subsequently, the new plane is\n","        delineated by reducing the width at the desired side and the respective vertical edge (height) at both of its\n","        adjacent corners.\n","    :param sign: Dictionary with sequential skeletal data of the signing person\n","    :param type: Type of shear augmentation to perform (either 'squeeze' or 'perspective')\n","    :param squeeze_ratio: Tuple containing the relative range from what the proportion of the original width will be\n","                          randomly chosen. These proportions will either be cut from both sides or used to construct the\n","                          new projection\n","    :return: Dictionary with augmented (by squeezing or perspective transformation) sequential skeletal data of the\n","             signing person\n","    \"\"\"\n","\n","    body_landmarks, hand_landmarks = __preprocess_row_sign(sign)\n","\n","    if type == \"squeeze\":\n","        move_left = random.uniform(*squeeze_ratio)\n","        move_right = random.uniform(*squeeze_ratio)\n","\n","        src = np.array(((0, 1), (1, 1), (0, 0), (1, 0)), dtype=np.float32)\n","        dest = np.array(((0 + move_left, 1), (1 - move_right, 1), (0 + move_left, 0), (1 - move_right, 0)),\n","                        dtype=np.float32)\n","        mtx = cv2.getPerspectiveTransform(src, dest)\n","\n","    elif type == \"perspective\":\n","\n","        move_ratio = random.uniform(*squeeze_ratio)\n","        src = np.array(((0, 1), (1, 1), (0, 0), (1, 0)), dtype=np.float32)\n","\n","        if __random_pass(0.5):\n","            dest = np.array(((0 + move_ratio, 1 - move_ratio), (1, 1), (0 + move_ratio, 0 + move_ratio), (1, 0)),\n","                            dtype=np.float32)\n","        else:\n","            dest = np.array(((0, 1), (1 - move_ratio, 1 - move_ratio), (0, 0), (1 - move_ratio, 0 + move_ratio)),\n","                            dtype=np.float32)\n","\n","        mtx = cv2.getPerspectiveTransform(src, dest)\n","\n","    else:\n","\n","        logging.error(\"Unsupported shear type provided.\")\n","        return {}\n","\n","    landmarks_array = __dictionary_to_numpy(body_landmarks)\n","    augmented_landmarks = cv2.perspectiveTransform(np.array(landmarks_array, dtype=np.float32), mtx)\n","\n","    augmented_zero_landmark = cv2.perspectiveTransform(np.array([[[0, 0]]], dtype=np.float32), mtx)[0][0]\n","    augmented_landmarks = np.stack([np.where(sub == augmented_zero_landmark, [0, 0], sub) for sub in augmented_landmarks])\n","\n","    body_landmarks = __numpy_to_dictionary(augmented_landmarks)\n","\n","    return __wrap_sign_into_row(body_landmarks, hand_landmarks)\n","\n","\n","def augment_arm_joint_rotate(sign: dict, probability: float, angle_range: tuple) -> dict:\n","    \"\"\"\n","    AUGMENTATION TECHNIQUE. The joint coordinates of both arms are passed successively, and the impending landmark is\n","    slightly rotated with respect to the current one. The chance of each joint to be rotated is 3:10 and the angle of\n","    alternation is a uniform random angle up to +-4 degrees. This simulates slight, negligible variances in each\n","    execution of a sign, which do not change its semantic meaning.\n","    :param sign: Dictionary with sequential skeletal data of the signing person\n","    :param probability: Probability of each joint to be rotated (float from the range [0, 1])\n","    :param angle_range: Tuple containing the angle range (minimal and maximal angle in degrees) to randomly choose the\n","                        angle by which the landmarks will be rotated from\n","    :return: Dictionary with augmented (by arm joint rotation) sequential skeletal data of the signing person\n","    \"\"\"\n","\n","    body_landmarks, hand_landmarks = __preprocess_row_sign(sign)\n","\n","    # Iterate over both directions (both hands)\n","    for side in [\"left\", \"right\"]:\n","        # Iterate gradually over the landmarks on arm\n","        for landmark_index, landmark_origin in enumerate(ARM_IDENTIFIERS_ORDER):\n","            landmark_origin = landmark_origin.replace(\"$side$\", side)\n","\n","            # End the process on the current hand if the landmark is not present\n","            if landmark_origin not in body_landmarks:\n","                break\n","\n","            # Perform rotation by provided probability\n","            if __random_pass(probability):\n","                angle = math.radians(random.uniform(*angle_range))\n","\n","                for to_be_rotated in ARM_IDENTIFIERS_ORDER[landmark_index + 1:]:\n","                    to_be_rotated = to_be_rotated.replace(\"$side$\", side)\n","\n","                    # Skip if the landmark is not present\n","                    if to_be_rotated not in body_landmarks:\n","                        continue\n","\n","                    body_landmarks[to_be_rotated] = [__rotate(body_landmarks[landmark_origin][frame_index], frame,\n","                        angle) for frame_index, frame in enumerate(body_landmarks[to_be_rotated])]\n","\n","    return __wrap_sign_into_row(body_landmarks, hand_landmarks)"]},{"cell_type":"markdown","metadata":{},"source":["# Adding Gausian Noise"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.682753Z","iopub.status.busy":"2024-07-17T12:50:03.682057Z","iopub.status.idle":"2024-07-17T12:50:03.694582Z","shell.execute_reply":"2024-07-17T12:50:03.693388Z","shell.execute_reply.started":"2024-07-17T12:50:03.682717Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","class GaussianNoise(object):\n","    def __init__(self, mean=0., std=1.):\n","        self.std = std\n","        self.mean = mean\n","\n","    def __call__(self, tensor):\n","        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.696487Z","iopub.status.busy":"2024-07-17T12:50:03.696174Z","iopub.status.idle":"2024-07-17T12:50:03.723135Z","shell.execute_reply":"2024-07-17T12:50:03.722065Z","shell.execute_reply.started":"2024-07-17T12:50:03.696461Z"},"trusted":true},"outputs":[],"source":["import ast\n","import torch\n","\n","import pandas as pd\n","import torch.utils.data as torch_data\n","\n","from random import randrange\n","\n","\n","def load_dataset(df):\n","\n","    # Load the datset csv file\n","#     df = pd.read_csv(file_location, encoding=\"utf-8\")\n","    HAND_IDENTIFIERS = [\n","    \"wrist\",\n","    \"indexTip\",\n","    \"indexDIP\",\n","    \"indexPIP\",\n","    \"indexMCP\",\n","    \"middleTip\",\n","    \"middleDIP\",\n","    \"middlePIP\",\n","    \"middleMCP\",\n","    \"ringTip\",\n","    \"ringDIP\",\n","    \"ringPIP\",\n","    \"ringMCP\",\n","    \"littleTip\",\n","    \"littleDIP\",\n","    \"littlePIP\",\n","    \"littleMCP\",\n","    \"thumbTip\",\n","    \"thumbIP\",\n","    \"thumbMP\",\n","    \"thumbCMC\"\n","]\n","    HAND_IDENTIFIERS = [id + \"_0\" for id in HAND_IDENTIFIERS] + [id + \"_1\" for id in HAND_IDENTIFIERS]\n","\n","    # TO BE DELETED\n","    df.columns = [item.replace(\"_Left_\", \"_0_\").replace(\"_Right_\", \"_1_\") for item in list(df.columns)]\n","    if \"neck_X\" not in df.columns:\n","        df[\"neck_X\"] = [0 for _ in range(df.shape[0])]\n","        df[\"neck_Y\"] = [0 for _ in range(df.shape[0])]\n","\n","    # TEMP\n","    labels = df[\"labels\"].to_list()\n","#     print(labels)\n","#     print(len(labels))\n","    # labels = [label + 1 for label in df[\"labels\"].to_list()]\n","    data = []\n","\n","    for row_index, row in df.iterrows():\n","        current_row = np.empty(shape=(len(ast.literal_eval(row[\"leftEar_X\"])), len(BODY_IDENTIFIERS + HAND_IDENTIFIERS), 2))\n","        for index, identifier in enumerate(BODY_IDENTIFIERS + HAND_IDENTIFIERS):\n","            current_row[:, index, 0] = ast.literal_eval(row[identifier + \"_X\"])\n","            current_row[:, index, 1] = ast.literal_eval(row[identifier + \"_Y\"])\n","        data.append(current_row)\n","\n","    return data, labels\n","\n","\n","def tensor_to_dictionary(landmarks_tensor: torch.Tensor) -> dict:\n","    HAND_IDENTIFIERS = [\n","    \"wrist\",\n","    \"indexTip\",\n","    \"indexDIP\",\n","    \"indexPIP\",\n","    \"indexMCP\",\n","    \"middleTip\",\n","    \"middleDIP\",\n","    \"middlePIP\",\n","    \"middleMCP\",\n","    \"ringTip\",\n","    \"ringDIP\",\n","    \"ringPIP\",\n","    \"ringMCP\",\n","    \"littleTip\",\n","    \"littleDIP\",\n","    \"littlePIP\",\n","    \"littleMCP\",\n","    \"thumbTip\",\n","    \"thumbIP\",\n","    \"thumbMP\",\n","    \"thumbCMC\"\n","]\n","    HAND_IDENTIFIERS = [id + \"_0\" for id in HAND_IDENTIFIERS] + [id + \"_1\" for id in HAND_IDENTIFIERS]\n","\n","    data_array = landmarks_tensor.numpy()\n","    output = {}\n","\n","    for landmark_index, identifier in enumerate(BODY_IDENTIFIERS + HAND_IDENTIFIERS):\n","        output[identifier] = data_array[:, landmark_index]\n","\n","    return output\n","\n","\n","def dictionary_to_tensor(landmarks_dict: dict) -> torch.Tensor:\n","    HAND_IDENTIFIERS = [\n","    \"wrist\",\n","    \"indexTip\",\n","    \"indexDIP\",\n","    \"indexPIP\",\n","    \"indexMCP\",\n","    \"middleTip\",\n","    \"middleDIP\",\n","    \"middlePIP\",\n","    \"middleMCP\",\n","    \"ringTip\",\n","    \"ringDIP\",\n","    \"ringPIP\",\n","    \"ringMCP\",\n","    \"littleTip\",\n","    \"littleDIP\",\n","    \"littlePIP\",\n","    \"littleMCP\",\n","    \"thumbTip\",\n","    \"thumbIP\",\n","    \"thumbMP\",\n","    \"thumbCMC\"\n","]\n","    HAND_IDENTIFIERS = [id + \"_0\" for id in HAND_IDENTIFIERS] + [id + \"_1\" for id in HAND_IDENTIFIERS]\n","\n","    output = np.empty(shape=(len(landmarks_dict[\"leftEar\"]), len(BODY_IDENTIFIERS + HAND_IDENTIFIERS), 2))\n","\n","    for landmark_index, identifier in enumerate(BODY_IDENTIFIERS + HAND_IDENTIFIERS):\n","        output[:, landmark_index, 0] = [frame[0] for frame in landmarks_dict[identifier]]\n","        output[:, landmark_index, 1] = [frame[1] for frame in landmarks_dict[identifier]]\n","\n","    return torch.from_numpy(output)\n","\n","\n","class CzechSLRDataset(torch_data.Dataset):\n","    \"\"\"Advanced object representation of the HPOES dataset for loading hand joints landmarks utilizing the Torch's\n","    built-in Dataset properties\"\"\"\n","\n","    data: [np.ndarray]\n","    labels: [np.ndarray]\n","\n","    def __init__(self, dataset_filename: str, num_labels=5, transform=None, augmentations=False,\n","                 augmentations_prob=0.5, normalize=True):\n","        \"\"\"\n","        Initiates the HPOESDataset with the pre-loaded data from the h5 file.\n","\n","        :param dataset_filename: Path to the h5 file\n","        :param transform: Any data transformation to be applied (default: None)\n","        \"\"\"\n","\n","        loaded_data = load_dataset(dataset_filename)\n","        data, labels = loaded_data[0], loaded_data[1]\n","#         print(\"CzechSLRDataset Loaded\",loaded_data)\n","#         print(\"CzechSLRDataset\",labels)\n","#         print(\"CzechSLRDataset\",len(labels))\n","\n","        self.data = data\n","        self.labels = np.array(labels).astype(int)\n","        self.targets = list(labels)\n","        self.num_labels = num_labels\n","        self.transform = transform\n","#         print(\"NPArray\",np.array(labels).astype(int))\n","#         print(\"NPArraylen\",len(np.array(labels).astype(int)))\n","#         self.label = torch.tensor(self.labels, dtype=torch.long)\n","#         print(\"Tensors torch.tensor\", label1[:])\n","        \n","        self.augmentations = augmentations\n","        self.augmentations_prob = augmentations_prob \n","        self.normalize = normalize\n","#         print(\"label size\", self.label.shape)\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Allocates, potentially transforms and returns the item at the desired index.\n","\n","        :param idx: Index of the item\n","        :return: Tuple containing both the depth map and the label\n","        \"\"\"\n","#         print(\"Data len before\",len(self.data))\n","#         print(type(self.labels    \n","#         print(self.labels)\n","        depth_map = torch.from_numpy(np.copy(self.data[idx]))\n","        label = torch.Tensor([self.labels[idx]])\n","#         label = self.label.unsqueeze(0)\n","#         print(\"label size\", label.shape)\n","#         print(\"Depth after\",depth_map[:])\n","#         print(\"Depth Len after\",len(depth_map))\n","          \n","#         label = torch.tensor(self.labels[idx], dtype=torch.long)\n","#         print(\"tensor converted\", label)\n","#         print(type(label))\n","#         print(len(label))\n","\n","        depth_map = tensor_to_dictionary(depth_map)\n","\n","        # Apply potential augmentations\n","        if self.augmentations and random.random() < self.augmentations_prob:\n","\n","            selected_aug = randrange(4)\n","\n","            if selected_aug == 0:\n","                depth_map = augment_rotate(depth_map, (-13, 13))\n","\n","            if selected_aug == 1:\n","                depth_map = augment_shear(depth_map, \"perspective\", (0, 0.1))\n","\n","            if selected_aug == 2:\n","                depth_map = augment_shear(depth_map, \"squeeze\", (0, 0.15))\n","\n","            if selected_aug == 3:\n","                depth_map = augment_arm_joint_rotate(depth_map, 0.3, (-4, 4))\n","\n","        if self.normalize:\n","            depth_map = normalize_single_body_dict(depth_map)\n","            depth_map = normalize_single_hand_dict(depth_map)\n","\n","        depth_map = dictionary_to_tensor(depth_map)\n","\n","        # Move the landmark position interval to improve performance\n","        depth_map = depth_map - 0.5\n","\n","        if self.transform:\n","            depth_map = self.transform(depth_map)\n","#         print(\"Data size\", depth_map.shape)\n","        return depth_map, label\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Architecture"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T13:02:13.543060Z","iopub.status.busy":"2024-07-17T13:02:13.542631Z","iopub.status.idle":"2024-07-17T13:02:13.560295Z","shell.execute_reply":"2024-07-17T13:02:13.558994Z","shell.execute_reply.started":"2024-07-17T13:02:13.542999Z"},"trusted":true},"outputs":[],"source":["import copy\n","import torch\n","\n","import torch.nn as nn\n","from typing import Optional\n","\n","\n","def _get_clones(mod, n):\n","    return nn.ModuleList([copy.deepcopy(mod) for _ in range(n)])\n","\n","\n","class SPOTERTransformerDecoderLayer(nn.TransformerDecoderLayer):\n","    \"\"\"\n","    Edited TransformerDecoderLayer implementation omitting the redundant self-attention operation as opposed to the\n","    standard implementation.\n","    \"\"\"\n","\n","    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n","        super(SPOTERTransformerDecoderLayer, self).__init__(d_model, nhead, dim_feedforward, dropout, activation)\n","\n","#         del self.self_attn\n","\n","    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None,\n","                memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None,\n","                memory_key_padding_mask: Optional[torch.Tensor] = None, **kwargs) -> torch.Tensor:\n","\n","        tgt = tgt + self.dropout1(tgt)\n","        tgt = self.norm1(tgt)\n","        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n","                                   key_padding_mask=memory_key_padding_mask)[0]\n","        tgt = tgt + self.dropout2(tgt2)\n","        tgt = self.norm2(tgt)\n","        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n","        tgt = tgt + self.dropout3(tgt2)\n","        tgt = self.norm3(tgt)\n","\n","        return tgt\n","\n","\n","class SPOTER(nn.Module):\n","    \"\"\"\n","    Implementation of the SPOTER (Sign POse-based TransformER) architecture for sign language recognition from sequence\n","    of skeletal data.\n","    \"\"\"\n","\n","    def __init__(self, num_classes, hidden_dim=55):\n","        super().__init__()\n","\n","        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim))\n","        self.pos = nn.Parameter(torch.cat([self.row_embed[0].unsqueeze(0).repeat(1, 1, 1)], dim=-1).flatten(0, 1).unsqueeze(0))\n","        self.class_query = nn.Parameter(torch.rand(1, hidden_dim))\n","        self.transformer = nn.Transformer(hidden_dim, 9, 6, 6)\n","        self.linear_class = nn.Linear(hidden_dim, num_classes)\n","\n","        # Deactivate the initial attention decoder mechanism\n","        custom_decoder_layer = SPOTERTransformerDecoderLayer(self.transformer.d_model, self.transformer.nhead, 2048,\n","                                                             0.1, \"relu\")\n","        self.transformer.decoder.layers = _get_clones(custom_decoder_layer, self.transformer.decoder.num_layers)\n","\n","    def forward(self, inputs):\n","        h = torch.unsqueeze(inputs.flatten(start_dim=1), 1).float()\n","        h = self.transformer(self.pos + h, self.class_query.unsqueeze(0)).transpose(0, 1)\n","        res = self.linear_class(h)\n","\n","        return res\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:50:03.742677Z","iopub.status.busy":"2024-07-17T12:50:03.742324Z","iopub.status.idle":"2024-07-17T12:50:03.756536Z","shell.execute_reply":"2024-07-17T12:50:03.755486Z","shell.execute_reply.started":"2024-07-17T12:50:03.742650Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","from collections import Counter\n","from torch.utils.data import Subset\n","from sklearn.model_selection import train_test_split\n","\n","\n","def __balance_val_split(dataset, val_split=0.):\n","    targets = np.array(dataset.targets)\n","    train_indices, val_indices = train_test_split(\n","        np.arange(targets.shape[0]),\n","        test_size=val_split,\n","        stratify=targets\n","    )\n","\n","    train_dataset = Subset(dataset, indices=train_indices)\n","    val_dataset = Subset(dataset, indices=val_indices)\n","\n","    return train_dataset, val_dataset\n","\n","\n","def __split_of_train_sequence(subset: Subset, train_split=1.0):\n","    if train_split == 1:\n","        return subset\n","\n","    targets = np.array([subset.dataset.targets[i] for i in subset.indices])\n","    train_indices, _ = train_test_split(\n","        np.arange(targets.shape[0]),\n","        test_size=1 - train_split,\n","        stratify=targets\n","    )\n","\n","    train_dataset = Subset(subset.dataset, indices=[subset.indices[i] for i in train_indices])\n","\n","    return train_dataset\n","\n","\n","def __log_class_statistics(subset: Subset):\n","    train_classes = [subset.dataset.targets[i] for i in subset.indices]\n","    print(dict(Counter(train_classes)))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T12:56:17.149998Z","iopub.status.busy":"2024-07-17T12:56:17.149573Z","iopub.status.idle":"2024-07-17T12:56:17.167142Z","shell.execute_reply":"2024-07-17T12:56:17.166105Z","shell.execute_reply.started":"2024-07-17T12:56:17.149967Z"},"trusted":true},"outputs":[],"source":["import logging\n","import torch\n","\n","\n","def train_epoch(model, dataloader, criterion, optimizer, device, scheduler=None):\n","\n","    pred_correct, pred_all = 0, 0\n","    running_loss = 0.0\n","\n","    for i, data in enumerate(dataloader):\n","        inputs, labels = data\n","        # print(\"input\", inputs.shape)\n","        # print(\"label\",labels[:])\n","        inputs = inputs.squeeze(0).to(device)\n","#         remove\n","#         inputs = inputs.view(inputs.size(0), inputs.size(1), -1)\n","        # batch_size, seq_length, spatial_dim1, spatial_dim2 = inputs.shape\n","        # inputs = inputs.view(batch_size, seq_length, -1)\n","        \n","#         \n","        labels = labels.to(device, dtype=torch.long)\n","        # print(\"input\",inputs.shape)\n","        \n","        optimizer.zero_grad()\n","        outputs = model(inputs).expand(1, -1, -1)\n","        # print(\"Shape of outputs after model:\", outputs.shape)\n","        # print(\"Shape of labels:\", labels.shape)\n","#         print(\"Output all\",outputs[:])\n","#         print(\"output 0\",outputs[0])\n","#         print(\"labels 0\", labels[0])\n","        loss = criterion(outputs[0], labels[0])\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss\n","\n","        # Statistics\n","        if int(torch.argmax(torch.nn.functional.softmax(outputs, dim=2))) == int(labels[0][0]):\n","            pred_correct += 1\n","        pred_all += 1\n","\n","    if scheduler:\n","        scheduler.step(running_loss.item() / len(dataloader))\n","        \n","    del inputs, labels, loss, outputs\n","    gc.collect()\n","\n","    return running_loss, pred_correct, pred_all, (pred_correct / pred_all)\n","\n","\n","def evaluate(model, dataloader, device, print_stats=False):\n","\n","    pred_correct, pred_all = 0, 0\n","    stats = {i: [0, 0] for i in range(compute_num_labels(df))}\n","\n","    for i, data in enumerate(dataloader):\n","        inputs, labels = data\n","        inputs = inputs.squeeze(0).to(device)\n","        labels = labels.to(device, dtype=torch.long)\n","\n","        outputs = model(inputs).expand(1, -1, -1)\n","\n","        # Statistics\n","        if int(torch.argmax(torch.nn.functional.softmax(outputs, dim=2))) == int(labels[0][0]):\n","            stats[int(labels[0][0])][0] += 1\n","            pred_correct += 1\n","\n","        stats[int(labels[0][0])][1] += 1\n","        pred_all += 1\n","\n","    if print_stats:\n","        stats = {key: value[0] / value[1] for key, value in stats.items() if value[1] != 0}\n","        print(\"Label accuracies statistics:\")\n","        print(str(stats) + \"\\n\")\n","        logging.info(\"Label accuracies statistics:\")\n","        logging.info(str(stats) + \"\\n\")\n","\n","    return pred_correct, pred_all, (pred_correct / pred_all)\n","\n","\n","def evaluate_top_k(model, dataloader, device, k=5):\n","\n","    pred_correct, pred_all = 0, 0\n","\n","    for i, data in enumerate(dataloader):\n","        inputs, labels = data\n","        inputs = inputs.squeeze(0).to(device)\n","        labels = labels.to(device, dtype=torch.long)\n","\n","        outputs = model(inputs).expand(1, -1, -1)\n","\n","        if int(labels[0][0]) in torch.topk(outputs, k).indices.tolist():\n","            pred_correct += 1\n","\n","        pred_all += 1\n","\n","    return pred_correct, pred_all, (pred_correct / pred_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T13:00:46.769354Z","iopub.status.busy":"2024-07-17T13:00:46.768945Z","iopub.status.idle":"2024-07-17T13:01:52.779966Z","shell.execute_reply":"2024-07-17T13:01:52.778889Z","shell.execute_reply.started":"2024-07-17T13:00:46.769323Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","import logging\n","import gc\n","from pathlib import Path\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","from tqdm import tqdm_notebook\n","\n","class Args:\n","    def __init__(self):\n","        self.experiment_name = \"lsa_64_spoter\"\n","        self.num_classes = compute_num_labels(df)\n","        self.hidden_dim = 108\n","        self.seed = 379\n","        self.dataset_path = df \n","        self.train_split_ratio = 0.8\n","        self.val_split_ratio = 0.1\n","        self.epochs = 100\n","        self.lr = 0.001\n","        self.log_freq = 1\n","        self.save_checkpoints = True\n","        self.scheduler_factor = 0.1\n","        self.scheduler_patience = 5\n","        self.gaussian_mean = 0\n","        self.gaussian_std = 0.001\n","        self.plot_stats = True\n","        self.plot_lr = True\n","\n","args = Args()\n","\n","        \n","# Initialize all the random seeds\n","random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.cuda.manual_seed(args.seed)\n","torch.backends.cudnn.deterministic = True\n","\n","# Set up logging\n","log_file = f\"{args.experiment_name}_{str(args.train_split_ratio).replace('.', '')}.log\"\n","logging.basicConfig(filename=log_file, level=logging.INFO,\n","                    format=\"%(asctime)s [%(levelname)s] %(message)s\")\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create directories for checkpoints and images if they don't exist\n","checkpoint_dir = f\"out-checkpoints/{args.experiment_name}/\"\n","image_dir = \"out-img/\"\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","os.makedirs(image_dir, exist_ok=True)\n","\n","# Initialize model, criterion, optimizer, scheduler\n","slrt_model = SPOTER(num_classes=args.num_classes, hidden_dim=args.hidden_dim).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(slrt_model.parameters(), lr=args.lr)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.scheduler_factor,\n","                                                 patience=args.scheduler_patience)\n","\n","# Load dataset and split\n","full_dataset = CzechSLRDataset(args.dataset_path)\n","train_size = int(args.train_split_ratio * len(full_dataset))\n","val_size = int(args.val_split_ratio * len(full_dataset))\n","test_size = len(full_dataset) - train_size - val_size\n","train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n","\n","# Define transformations\n","transform_train = transforms.Compose([GaussianNoise(args.gaussian_mean, args.gaussian_std)])\n","transform_test = None\n","\n","# Apply transformations to datasets\n","train_set.dataset.transform = transform_train\n","train_set.dataset.augmentations = True\n","val_set.dataset.transform = transform_test\n","val_set.dataset.augmentations = False\n","test_set.dataset.transform = transform_test\n","test_set.dataset.augmentations = False\n","\n","# Create data loaders\n","train_loader = DataLoader(train_set) \n","val_loader = DataLoader(val_set)  \n","test_loader = DataLoader(test_set)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-17T13:01:55.472196Z","iopub.status.busy":"2024-07-17T13:01:55.471244Z","iopub.status.idle":"2024-07-17T13:01:56.549672Z","shell.execute_reply":"2024-07-17T13:01:56.547244Z","shell.execute_reply.started":"2024-07-17T13:01:55.472163Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_63504/3191850146.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","\n","  for epoch in tqdm_notebook(range(args.epochs), desc=\"Training\", total=args.epochs, unit='epoch'):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c64456984a414ad8861e26714663873e","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/100 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [1/100], Train Loss: 16276.2256, Train Acc: 0.0034, Val Acc: 0.0000\n","\n","Saved checkpoint at epoch 2 with Train accuracy 0.011213047910295617 validation accuracy 0.0326975476839237\n","\n","Epoch [2/100], Train Loss: 15678.1621, Train Acc: 0.0112, Val Acc: 0.0327\n","\n","Saved checkpoint at epoch 3 with Train accuracy 0.03431872239211689 validation accuracy 0.0653950953678474\n","\n","Epoch [3/100], Train Loss: 14524.7402, Train Acc: 0.0343, Val Acc: 0.0654\n","\n","Saved checkpoint at epoch 4 with Train accuracy 0.08053007135575943 validation accuracy 0.1362397820163488\n","\n","Epoch [4/100], Train Loss: 13243.6338, Train Acc: 0.0805, Val Acc: 0.1362\n","\n","Saved checkpoint at epoch 5 with Train accuracy 0.15664288141352362 validation accuracy 0.20435967302452315\n","\n","Epoch [5/100], Train Loss: 11861.1406, Train Acc: 0.1566, Val Acc: 0.2044\n","\n","Saved checkpoint at epoch 6 with Train accuracy 0.22663948352021746 validation accuracy 0.24250681198910082\n","\n","Epoch [6/100], Train Loss: 10543.5879, Train Acc: 0.2266, Val Acc: 0.2425\n","\n","Saved checkpoint at epoch 7 with Train accuracy 0.31022765885151204 validation accuracy 0.30517711171662126\n","\n","Epoch [7/100], Train Loss: 9319.0293, Train Acc: 0.3102, Val Acc: 0.3052\n","\n","Saved checkpoint at epoch 8 with Train accuracy 0.3931362555215766 validation accuracy 0.3814713896457766\n","\n","Epoch [8/100], Train Loss: 8172.1821, Train Acc: 0.3931, Val Acc: 0.3815\n","\n","Saved checkpoint at epoch 9 with Train accuracy 0.4709480122324159 validation accuracy 0.4550408719346049\n","\n","Epoch [9/100], Train Loss: 7076.4644, Train Acc: 0.4709, Val Acc: 0.4550\n","\n","Saved checkpoint at epoch 10 with Train accuracy 0.5283724091063541 validation accuracy 0.4822888283378747\n","\n","Epoch [10/100], Train Loss: 6134.8154, Train Acc: 0.5284, Val Acc: 0.4823\n","\n","Saved checkpoint at epoch 11 with Train accuracy 0.5956506965681277 validation accuracy 0.5449591280653951\n","\n","Epoch [11/100], Train Loss: 5291.5352, Train Acc: 0.5957, Val Acc: 0.5450\n","\n","Saved checkpoint at epoch 12 with Train accuracy 0.6591913013931363 validation accuracy 0.6158038147138964\n","\n","Epoch [12/100], Train Loss: 4563.0698, Train Acc: 0.6592, Val Acc: 0.6158\n","\n","Saved checkpoint at epoch 13 with Train accuracy 0.7108392796466191 validation accuracy 0.6348773841961853\n","\n","Epoch [13/100], Train Loss: 3907.1689, Train Acc: 0.7108, Val Acc: 0.6349\n","\n","Saved checkpoint at epoch 14 with Train accuracy 0.7584097859327217 validation accuracy 0.6730245231607629\n","\n","Epoch [14/100], Train Loss: 3351.1204, Train Acc: 0.7584, Val Acc: 0.6730\n","\n","Saved checkpoint at epoch 15 with Train accuracy 0.7954468229697588 validation accuracy 0.6893732970027248\n","\n","Epoch [15/100], Train Loss: 2882.1748, Train Acc: 0.7954, Val Acc: 0.6894\n","\n","Saved checkpoint at epoch 16 with Train accuracy 0.8188922867821951 validation accuracy 0.7356948228882834\n","\n","Epoch [16/100], Train Loss: 2492.5374, Train Acc: 0.8189, Val Acc: 0.7357\n","\n","Saved checkpoint at epoch 17 with Train accuracy 0.852871219843697 validation accuracy 0.7438692098092643\n","\n","Epoch [17/100], Train Loss: 2125.2871, Train Acc: 0.8529, Val Acc: 0.7439\n","\n","Saved checkpoint at epoch 18 with Train accuracy 0.8773360516479782 validation accuracy 0.7602179836512262\n","\n","Epoch [18/100], Train Loss: 1829.7452, Train Acc: 0.8773, Val Acc: 0.7602\n","\n","Saved checkpoint at epoch 19 with Train accuracy 0.9028202514441046 validation accuracy 0.7901907356948229\n","\n","Epoch [19/100], Train Loss: 1569.0121, Train Acc: 0.9028, Val Acc: 0.7902\n","\n","Saved checkpoint at epoch 20 with Train accuracy 0.9147128780156303 validation accuracy 0.7956403269754768\n","\n","Epoch [20/100], Train Loss: 1359.7177, Train Acc: 0.9147, Val Acc: 0.7956\n","\n","Saved checkpoint at epoch 21 with Train accuracy 0.9249065579340808 validation accuracy 0.8147138964577657\n","\n","Epoch [21/100], Train Loss: 1171.8833, Train Acc: 0.9249, Val Acc: 0.8147\n","\n","Saved checkpoint at epoch 22 with Train accuracy 0.9388379204892966 validation accuracy 0.8337874659400545\n","\n","Epoch [22/100], Train Loss: 1026.1388, Train Acc: 0.9388, Val Acc: 0.8338\n","\n","Saved checkpoint at epoch 23 with Train accuracy 0.9500509683995922 validation accuracy 0.8419618528610354\n","\n","Epoch [23/100], Train Loss: 898.4197, Train Acc: 0.9501, Val Acc: 0.8420\n","\n","Saved checkpoint at epoch 24 with Train accuracy 0.9541284403669725 validation accuracy 0.8555858310626703\n","\n","Epoch [24/100], Train Loss: 793.7397, Train Acc: 0.9541, Val Acc: 0.8556\n","\n","Epoch [25/100], Train Loss: 716.4247, Train Acc: 0.9609, Val Acc: 0.8529\n","\n","Saved checkpoint at epoch 26 with Train accuracy 0.9609242269792728 validation accuracy 0.8583106267029973\n","\n","Epoch [26/100], Train Loss: 637.0698, Train Acc: 0.9609, Val Acc: 0.8583\n","\n","Epoch [27/100], Train Loss: 567.3700, Train Acc: 0.9698, Val Acc: 0.8583\n","\n","Epoch [28/100], Train Loss: 513.9265, Train Acc: 0.9708, Val Acc: 0.8420\n"]}],"source":["# Training loop\n","train_losses = []\n","train_accs = []\n","val_accs = []\n","# lr_progress = []\n","best_val_acc = 0.0\n","\n","for epoch in tqdm_notebook(range(args.epochs), desc=\"Training\", total=args.epochs, unit='epoch'):\n","    slrt_model.train()\n","    train_loss, _, _, train_acc = train_epoch(slrt_model, train_loader, criterion, optimizer, device)\n","    train_losses.append(train_loss)\n","    train_accs.append(train_acc)\n","\n","    slrt_model.eval()\n","    with torch.no_grad():\n","        _, _, val_acc = evaluate(slrt_model, val_loader, device)\n","    val_accs.append(val_acc)\n","\n","    # Update learning rate scheduler\n","    scheduler.step(train_loss)\n","\n","    # Save checkpoints\n","    if args.save_checkpoints and val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        checkpoint_path = f\"{checkpoint_dir}/checkpoint_{epoch}.pth\"\n","        torch.save(slrt_model.state_dict(), checkpoint_path)\n","        print(f\"Saved checkpoint at epoch {epoch+1} with Train accuracy {train_acc} validation accuracy {val_acc}\")\n","        logging.info(f\"Saved checkpoint at epoch {epoch+1} with validation accuracy {val_acc}\")\n","\n","    # Logging and progress display\n","    if (epoch + 1) % args.log_freq == 0:\n","        print(f\"Epoch [{epoch + 1}/{args.epochs}], \"\n","              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n","              f\"Val Acc: {val_acc:.4f}\")\n","        logging.info(f\"Epoch [{epoch + 1}/{args.epochs}], \"\n","                     f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n","                     f\"Val Acc: {val_acc:.4f}\")\n","        \n","    # Explicitly delete variables and run garbage collection\n","    del train_loss, train_acc, val_acc\n","    gc.collect()\n","    # Clear CUDA cache to prevent out-of-memory issues\n","    torch.cuda.empty_cache() \n","\n","#     lr_progress.append(optimizer.param_groups[0][\"lr\"])\n","\n","# Return the training history\n","return train_losses, train_accs, val_accs\n","\n","train_losses, train_accs, val_accs, lr_progress = train(args)"]},{"cell_type":"markdown","metadata":{},"source":["# PLotting"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-17T12:50:10.925176Z","iopub.status.idle":"2024-07-17T12:50:10.926141Z","shell.execute_reply":"2024-07-17T12:50:10.925850Z","shell.execute_reply.started":"2024-07-17T12:50:10.925824Z"},"trusted":true},"outputs":[],"source":["def plot_results(args, train_losses, train_accs, val_accs, lr_progress):\n","    # Plotting\n","    image_dir = \"out-img/\"\n","    \n","    plt.figure(figsize=(10, 6))\n","    plt.plot(range(1, args.epochs + 1), [acc for acc in train_accs], label=\"Train Accuracy\")\n","    plt.plot(range(1, args.epochs + 1), [acc for acc in val_accs], label=\"Validation Accuracy\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Metrics\")\n","    plt.title(\"Training and Validation Metrics\")\n","    plt.legend()\n","    plt.grid()\n","    \n","    if args.plot_stats:\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(range(1, args.epochs + 1), [loss.cpu().detach().numpy() for loss in train_losses], label=\"Train Loss\")\n","#         plt.plot(range(1, args.epochs + 1), [acc for acc in train_accs], label=\"Train Accuracy\")\n","#         plt.plot(range(1, args.epochs + 1), [acc for acc in val_accs], label=\"Validation Accuracy\")\n","        plt.xlabel(\"Epochs\")\n","        plt.ylabel(\"Metrics\")\n","        plt.title(\"Training and Validation Metrics\")\n","        plt.legend()\n","        plt.grid()\n","#         plt.savefig(f\"{image_dir}/{args.experiment_name}_metrics.png\")\n","#         plt.close()\n","\n","\n","    if args.plot_lr:\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(range(1, args.epochs + 1), [lr for lr in lr_progress], label=\"Learning Rate\")\n","        plt.xlabel(\"Epochs\") \n","        plt.ylabel(\"Learning Rate\")\n","        plt.title(\"Learning Rate Progress\")\n","        plt.legend()\n","        plt.grid()\n","#         plt.savefig(f\"{image_dir}/{args.experiment_name}_lr.png\")\n","#         plt.close()\n","\n","\n","plot_results(args, train_losses, train_accs, val_accs, lr_progress)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3852662,"sourceId":6677809,"sourceType":"datasetVersion"},{"datasetId":5359809,"sourceId":8913258,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
